{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ca3ccf5-f344-476f-ab88-d2a40b6de115",
   "metadata": {},
   "source": [
    "# ⚙️ Implementasi Scraper Artikel Wikipedia dengan `requests` dan `BeautifulSoup` di Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61af0a41-c82f-444d-99a6-d6f3bee5a764",
   "metadata": {},
   "source": [
    "Berikut adalah skrip Python untuk mengambil dan memproses konten artikel Wikipedia:\n",
    "\n",
    "* **Fungsi `get_wikipedia_page(topic)`:** Membuat URL Wikipedia berdasarkan topik, mengirimkan permintaan GET, dan mengembalikan konten HTML jika berhasil (status kode 200). Menangani kesalahan jika pengambilan data gagal.\n",
    "* **Fungsi `get_article_title(soup)`:** Menerima objek BeautifulSoup dan mengekstrak teks dari tag `<h1>` yang berisi judul artikel.\n",
    "* **Fungsi `get_article_summary(soup)`:** Menerima objek BeautifulSoup dan mencari paragraf `<p>` pertama yang tidak kosong untuk dijadikan ringkasan.\n",
    "* **Fungsi `get_headings(soup)`:** Menerima objek BeautifulSoup dan mengekstrak teks dari semua tag `<h2>`, `<h3>`, dan `<h4>` untuk mendapatkan heading artikel.\n",
    "* **Fungsi `get_related_links(soup)`:** Menerima objek BeautifulSoup, mencari tag `<a>` dengan atribut `href` yang dimulai dengan `/wiki/` dan tidak mengandung \":\", lalu mengembalikan hingga 5 tautan unik terkait artikel.\n",
    "* **Fungsi `main()`:**\n",
    "    * Meminta pengguna memasukkan topik pencarian.\n",
    "    * Memanggil `get_wikipedia_page()` untuk mendapatkan konten halaman.\n",
    "    * Jika konten berhasil diambil, membuat objek BeautifulSoup untuk parsing HTML.\n",
    "    * Memanggil fungsi-fungsi ekstraksi untuk mendapatkan judul, ringkasan, heading, dan tautan terkait.\n",
    "    * Menampilkan informasi yang diekstrak.\n",
    "* **Blok `if __name__ == \"__main__\":`:** Memastikan fungsi `main()` hanya dijalankan saat skrip dieksekusi secara langsung.\n",
    "\n",
    "Jalankan sel kode di bawah untuk menjelajahi informasi dari Wikipedia!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5434e15-6277-4a11-95aa-1beccc07cdea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a topic to search on Wikipedia:  Python\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Wikipedia Article Details ---\n",
      "\n",
      "Title: Python\n",
      "\n",
      "Summary: Python may refer to:\n",
      "\n",
      "Headings:\n",
      "- Contents\n",
      "- Snakes\n",
      "- Computing\n",
      "- People\n",
      "- Roller coasters\n",
      "\n",
      "Related Links:\n",
      "- https://en.wikipedia.org/wiki/Main_Page\n",
      "- https://en.wikipedia.org/wiki/Python_(genus)\n",
      "- https://en.wikipedia.org/wiki/CMU_Common_Lisp\n",
      "- https://en.wikipedia.org/wiki/Python_(nuclear_primary)\n",
      "- https://en.wikipedia.org/wiki/Python_(Efteling)\n"
     ]
    }
   ],
   "source": [
    "# Wikipedia Article Scraper\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Step 1: Get Wikipedia Article URL\n",
    "def get_wikipedia_page(topic):\n",
    "  url = f\"https://en.wikipedia.org/wiki/{topic.replace(' ', '_')}\"\n",
    "  response = requests.get(url)\n",
    "  if response.status_code == 200:\n",
    "    return response.text\n",
    "  else:\n",
    "    print(f\"Failed to retrieve data. Status code: {response.status_code}. Check the topic and try again\")\n",
    "    return None\n",
    "\n",
    "# Step 2: Extract Article Title\n",
    "def get_article_title(soup):\n",
    "  return soup.find('h1').text\n",
    "\n",
    "# Step 3: Extract Article Summary\n",
    "def get_article_summary(soup):\n",
    "  paragraphs = soup.find_all('p')\n",
    "  for para in paragraphs:\n",
    "    if para.text.strip():\n",
    "      return para.text.strip()\n",
    "  return \"No summary found\"\n",
    "\n",
    "# Step 4: Extract Headings\n",
    "def get_headings(soup):\n",
    "  headings = [heading.text.strip() for heading in soup.find_all(['h2', 'h3', 'h4'])]\n",
    "  return headings\n",
    "\n",
    "# Step 5: Extract Related Links\n",
    "def get_related_links(soup):\n",
    "  links = []\n",
    "  for a_tag in soup.find_all('a', href=True):\n",
    "    href = a_tag['href']\n",
    "    if href.startswith('/wiki/') and \":\" not in href:\n",
    "      links.append(f\"https://en.wikipedia.org{href}\")\n",
    "  return list(set(links))[:5]\n",
    "\n",
    "# Step 6: Main Program\n",
    "def main():\n",
    "  topic = input(\"Enter a topic to search on Wikipedia: \").strip()\n",
    "  page_content = get_wikipedia_page(topic)\n",
    "\n",
    "  if page_content:\n",
    "    soup = BeautifulSoup(page_content, 'html.parser')\n",
    "    title = get_article_title(soup)\n",
    "    summary = get_article_summary(soup)\n",
    "    headings = get_headings(soup)\n",
    "    related_links = get_related_links(soup)\n",
    "\n",
    "    print(\"\\n--- Wikipedia Article Details ---\")\n",
    "    print(f\"\\nTitle: {title}\")\n",
    "    print(f\"\\nSummary: {summary}\")\n",
    "    print(\"\\nHeadings:\")\n",
    "    for heading in headings[:5]:\n",
    "      print(f\"- {heading}\")\n",
    "\n",
    "    print(\"\\nRelated Links:\")\n",
    "    for link in related_links:\n",
    "      print(f\"- {link}\")\n",
    "\n",
    "# Run Program\n",
    "if __name__ == \"__main__\":\n",
    "  main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
